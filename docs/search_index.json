[
["expected-values-variance-etc-.html", "Time Series Midterm Review Unit 1 Expected Values, Variance, etc. 1.1 Notation and Some Definitions 1.2 Population Means and Variances 1.3 Estimating Mean and Variance for Multiple Realizations 1.4 Expected Value 1.5 Variance and Covariance", " Time Series Midterm Review David Josephs 2019-06-27 Unit 1 Expected Values, Variance, etc. 1.1 Notation and Some Definitions In time series, we denote the response variable as \\(X_t\\), and clearly the explanatory variable as \\(t\\). To denote a specific point in time, let us say \\(t = 9\\), we would say \\(X_9\\). The collection of all \\(X_n\\) is a stochastic process known as a time series. 1.1.1 Realizations and Ensembles A realization is a particular instance of a time series. It is one of the infinitely many possible time series that could have been observed. As a time series is a stochastic process, each one could be entirely different. The realization Is the one that was actually observed. Sometimes a realization is the only realization possible. We can theorize what an infinite population of those realizations, but it is often the case that there is only one. An ensemble is the theoretical totality of all possible realizations. It is the ensemble of realizations. We can think of that as the population of realizations. When we are thinking of means and variances, we think of them in terms of the ensemble, even though we may only be able to look at a single realization. 1.2 Population Means and Variances It is important to note that when dealing with an ensemble of realizations, each time point is allowed to have its own mean and variance. 1.2.1 The Mean of \\(X_t\\) \\(\\mu_t\\) is the mean of all possible realizations of \\(X_t\\) for a fixed t. 1.2.2 Variance of \\(X_t\\) \\({\\sigma_t}^{2}\\) is the variance of all possible realizations of \\(X_t\\) for a fixed t 1.3 Estimating Mean and Variance for Multiple Realizations Let us say we have \\(n\\) realizations of \\(X_t\\) at time \\(t\\). The average can be then written as \\(\\hat{\\mu_t}=\\frac{1}{n}\\sum_{i=1}^{n}X_{t_n}\\) Variance is calculated the same way. We are making sample means and sample variances. If we assume the population is normally distributed, this allows us to take confidence intervals on the mean of our set of realizations. However, We have a problem: What to do when we only have one realization? 1.4 Expected Value In short, the expected value \\((E[X])\\) of a random variable (RV) denoted by \\(X\\) is the mean or more intuitively the long run average of the event that variable represents. 1.4.1 Discrete RVs A discrete RV is an RV in which \\(X\\) cannot take on any value, it has specific values it can exist at and that is it. The formula for EV of a discrete RV is as follows: \\[E\\left[X\\right] = \\sum X P\\left(X\\right) = \\mu\\] 1.4.2 Continuous RVs A continuous RV has instead of discrete values a probability distribution/density function, \\(f(x)\\). The formula for EV of a continuous RV is as follows: \\[E\\left[X\\right] = \\int_{a}^{b} xf\\left(x\\right)dx = \\mu\\] Note that is directly analagous to the discrete RV, and that a and b can span to \\(\\pm\\infty\\) Important Note \\[E\\left[X\\right] = \\mu\\] That is the expected value of a RV is equivalent to the population mean 1.4.3 Some Rules Let a and b be constants \\[E\\left[a\\right] = a\\] \\[E[aX] = aE[X] = a\\mu\\] \\[E[aX + b] = aE[X] + E[b] = aE[x] + b = a\\mu+b\\] Knowing these rules, lets try out a challenge: 1.4.4 A challenge in Expected Values Find \\(E[\\bar{x}]\\) where \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}{x_i}\\) E.G find the expected value of an average 1.4.5 A solution First, we can factor out the 1/n, so we have \\[\\frac{1}{n}E\\left[\\sum_{i=1}^{n}{x_i}\\right]\\] then we can say that the sum is just \\(x_1+x_2+x_3+ ... + x_n\\). This would lead us to the conclusion that \\[ E[\\bar{x}] =\\frac{1}{n}\\sum_{i=1}^{n}E\\left[x_i\\right]\\] If we let each \\(x_i\\) be a constant then we have: \\[ E[\\bar{x}] =\\frac{1}{n}\\sum_{i=1}^{n}x_i = \\bar{x}\\] This makes sense as \\(\\bar{x}\\) is the average value of x, and so the expected value of x for a very high n is the average value of x 1.5 Variance and Covariance 1.5.1 Variance The variance is the dispersion. Variance is defined as \\[\\mathrm{Var}\\left(X\\right) = E \\left[\\left(X-\\mu\\right)^2\\right] = \\int_{-\\infty}^{\\infty} \\left(x - \\mu\\right)^{2}f\\left(x\\right)dx = \\sigma^2\\] \\[\\widehat{\\mathrm{Var}\\left(X\\right) }= \\sum\\left(x_i - \\bar{x}\\right)^2\\] 1.5.2 Covariance Assume we have two RVs, \\(X\\) and \\(Y\\): \\[\\mathrm{Cov}(X,Y) = E\\left[\\left(X - \\mu_x\\right)\\left(Y - \\mu_y\\right)\\right]\\] \\[\\widehat{\\mathrm{Cov}\\left(X,Y\\right) }= \\sum\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)\\] The variance is, in essence, the sum of cross products. 1.5.3 Some generalizations on covariance: If y tends to increase with x, then \\(\\mathrm{Cov}(X,Y)&gt;0\\) If Y tends to decrease with X, then \\(\\mathrm{Cov}(X,Y)\\) is Less than 0 If it appears as a random cloud, then \\(\\mathrm{Cov}(X,Y)\\) is approximately 0 1.5.4 A challenge Let \\(X\\) be a RV. What is \\(\\mathrm{Cov}(X,X)\\)? 1.5.5 A solution \\[\\mathrm{Cov}(X,X) = E\\left[\\left(X - \\mu_x\\right)\\left(X - \\mu_X\\right)\\right]\\] We see that the arguments in the left and right parens are exactly the same. Thus, we can rewrite the equation as: \\[\\mathrm{Cov}(X,X) = E \\left[\\left(X - \\mu_x\\right)^2\\right] = \\mathrm{Var}(X) \\] 1.5.6 A note on covariance and correlation Correlation is covariance but standardized. \\[\\mathrm{Corr}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{SD}(X)\\mathrm{SD}(Y)}\\] \\[ =\\frac{E[X - \\mu_x)(Y-\\mu_y)]}{\\sigma_x \\sigma_y}\\] \\[ \\widehat{\\mathrm{Corr}(X,Y)} = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{ns_x s_y}\\] "],
["a-brief-discussion-of-stationarity.html", "Unit 2 A Brief Discussion of Stationarity 2.1 Condition One: Constant Mean 2.2 Condition Two: Constant Variance 2.3 Condition 3: Constant autocorrelation", " Unit 2 A Brief Discussion of Stationarity In order for a time series to be considered stationary, it must satisfy three conditions: Constant Mean with Time Constant Variance with Time Constant Autocorellation with Time 2.1 Condition One: Constant Mean Mean is constant with time. That is \\(E[X_t] = \\mu\\). Note the lack of a little tiny t in mu! It is indepoent of tine! That is stationarity condition number 1! Important result: If we assume constant mean, we can use all of the data to estimate the mean Here we can see after 5 realizations that the mean is clearly not constant with time library(tswge) a1 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(3,0), vara = 3, plot =F) b1 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(3,0), vara = 3, plot =F) c1 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(3,0), vara = 3, plot =F) d1 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(3,0), vara = 3, plot =F) e1 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(3,0), vara = 3, plot =F) Now lets look at 5 realizations which come from the same realization, which might not have a constant mean: a2 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(runif(1,0,2*pi),0), vara = 3, plot =F) b2 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(runif(1,0,2*pi),0), vara = 3, plot =F) c2 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(runif(1,0,2*pi),0), vara = 3, plot =F) d2 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(runif(1,0,2*pi),0), vara = 3, plot =F) e2 = gen.sigplusnoise.wge(100,coef=c(5,0), freq = c(.1,0), psi = c(runif(1,0,2*pi),0), vara = 3, plot =F) We cannot say for sure whether the mean is constant or not in this case! Let us compare both of them on the same plot: par(mfrow = c(2,1)) plot(a1,type = &#39;l&#39;) lines(b1,col = &#39;blue&#39;, type = &#39;l&#39;) lines(c1,col = &#39;red&#39;, type = &#39;l&#39;) lines(d1,col = &#39;green&#39;, type = &#39;l&#39;) lines(e1, col =&#39;purple&#39;, type = &#39;l&#39;) plot(a2,type = &#39;l&#39;) lines(b2,col = &#39;blue&#39;, type = &#39;l&#39;) lines(c2,col = &#39;red&#39;, type = &#39;l&#39;) lines(d2,col = &#39;green&#39;, type = &#39;l&#39;) lines(e2, col =&#39;purple&#39;, type = &#39;l&#39;) par(mfrow = c(1,1)) 2.2 Condition Two: Constant Variance Variance does not depend on time Variance is constant and finite* That is : \\[Var[X_t] = \\sigma^2 &lt; \\infty\\] &gt; If we can make this assumption, then we can use all of the data to make the variance. This is typically the hardest one to tell, and it will take some practice to see. 2.3 Condition 3: Constant autocorrelation Correlation of \\(X_{t_1} and X_{t_2}\\) only depends on \\(t_2 - t_1\\) That is, the correlation between points depends only on how far apart they are in time, not where they are in time. To describe this mathematically: Let \\[h = t_2 - t_1\\] then \\[\\mathrm{Cor}\\left(X_t,X_{t+h}\\right) = \\rho_h\\] \\(\\rho\\) represents the population correlation coefficient. "],
["autocorrelation.html", "Unit 3 Autocorrelation 3.1 Independence 3.2 Serial Dependence / Autocorrelation", " Unit 3 Autocorrelation library(ggplot2) acfdf &lt;- function(vec){ vacf &lt;- acf(vec, plot = F) with(vacf, data.frame(lag,acf)) } ggacf &lt;- function(vec){ ac &lt;- acfdf(vec) ggplot(data = ac, aes(x = lag, y = acf))+ geom_hline(aes(yintercept = 0)) + geom_segment(mapping = aes(xend = lag, yend = 0)) } tplot &lt;- function(vec){ df &lt;- data.frame(&quot;X&quot; = vec, &quot;t&quot; = seq_along(vec)) ggplot(data = df, aes(x = t, y = X)) + geom_point() + geom_line() } 3.1 Independence Two events are independent in a time series if the probability that an event at time t occurs in no way depends on the ocurrence of any event in the past or affects any event in the future. Mathematically, this is written as: \\[\\mathrm{Independence: }P \\left(x_{t+1}|X_t\\right) = P\\left(X_{t+1}\\right)\\] 3.2 Serial Dependence / Autocorrelation 3.2.1 A definition If two events are independent, their corellation is 0 That is if \\(X_t\\) and \\(X_{t+k}\\) are independent, \\(\\rho_{x_{t},x_{t+k}}=0\\) Corollary: If the correlation between two variables is not zero, then they are not independent In other words if \\(\\rho_{x_{t},x_{t+k}} \\neq 0\\), they are not independent. 3.2.2 Autocorrelation Plots 3.2.2.1 Dependent(ish) data In time series we look at the autocorrelation between \\(X_t\\) and \\(X_{t+1}\\) etc (with t and t+1 it is lag 1 autocorrelation) For example, visually, let us define a vector, \\(Y5\\) and take its autocorrelation: Y5 &lt;- c(5.1,5.2,5.5,5.3,5.1,4.8,4.5,4.4,4.6,4.6,4.8,5.2,5.4,5.6,5.4,5.3,5.1,5.1,4.8,4.7,4.5,4.3,4.6,4.8,4.9,5.2,5.4,5.6,5.5,5.5) knitr::kable(acfdf(Y5)) lag acf 0 1.0000000 1 0.8268482 2 0.5166514 3 0.1125421 4 -0.2687774 5 -0.5538901 6 -0.6935454 7 -0.6979912 8 -0.5505557 9 -0.2752524 10 -0.0056082 11 0.2274804 12 0.3902111 13 0.4573468 14 0.4660549 tplot(Y5) ggacf(Y5) 3.2.2.2 Independent(ish) data! Now let us look at autocorrelation of independent data Realization1 = gen.arma.wge(n = 250) tplot(Realization1) ggacf(Realization1) Now lets look at the actual value of the autocorrelation knitr::kable(acfdf(Realization1)) lag acf 0 1.0000000 1 0.0252109 2 0.0056813 3 0.0777996 4 0.0578182 5 0.1228608 6 0.0191629 7 -0.0594078 8 0.0631660 9 -0.0423113 10 0.0574002 11 0.0011571 12 0.0321440 13 -0.0008658 14 -0.0665983 15 0.0461283 16 0.0039104 17 -0.1104903 18 -0.0468451 19 -0.0778730 20 0.0564766 21 -0.1828630 22 -0.0477041 23 -0.0640637 We see that \\[ \\hat{\\rho}\\left(t,t+1\\right) = \\hat{\\rho}_1 = r_1 = 0.0362456 \\] Which is more or less zero "],
["autocorrelation-concepts-and-notation.html", "Unit 4 Autocorrelation Concepts and Notation 4.1 Theoretical concept of \\(\\rho\\) 4.2 autocovariance 4.3 autocorrelation", " Unit 4 Autocorrelation Concepts and Notation 4.1 Theoretical concept of \\(\\rho\\) \\[\\rho = \\frac{E\\left[\\left(X-\\mu_X\\right)\\left(Y-\\mu_Y\\right)\\right]}{\\sigma_X \\sigma_Y} = \\frac{\\mathrm{covariance between X+y}}{\\sigma_X \\sigma_Y}\\] In a stationary time series, we have autocorrelation and autocovariance 4.2 autocovariance \\[\\gamma_k = E\\left[\\left(X_t - \\mu\\right)\\left(X_{t+k} - \\mu\\right)\\right]\\] 4.3 autocorrelation \\[\\rho_k =\\frac{E\\left[\\left(X_t - \\mu\\right)\\left(X_{t+k} - \\mu\\right)\\right]}{\\sigma_{X_t} \\sigma_{X_{t+k}}} = \\frac{E\\left[\\left(X_t - \\mu\\right)\\left(X_{t+k} - \\mu\\right)\\right]}{\\sigma^2_X} = \\frac{\\gamma_k}{\\sigma_X^2}\\] This works because of constant variance and constant mean! Note that in a stationary time series: \\[\\sigma_X^2 = 1E[(X_t - \\mu)^2] = E[(X_t -\\mu )(X_t - \\mu)] = \\gamma_0\\] Therefore: \\[\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\\] "]
]
