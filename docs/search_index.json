[
["expected-values-variance-etc-.html", "Time Series Midterm Review Unit 1 Expected Values, Variance, etc. 1.1 Notation and Some Definitions 1.2 Population Means and Variances 1.3 Estimating Mean and Variance for Multiple Realizations 1.4 Expected Value 1.5 Variance and Covariance", " Time Series Midterm Review David Josephs 2019-06-27 Unit 1 Expected Values, Variance, etc. 1.1 Notation and Some Definitions In time series, we denote the response variable as \\(X_t\\), and clearly the explanatory variable as \\(t\\). To denote a specific point in time, let us say \\(t = 9\\), we would say \\(X_9\\). The collection of all \\(X_n\\) is a stochastic process known as a time series. 1.1.1 Realizations and Ensembles A realization is a particular instance of a time series. It is one of the infinitely many possible time series that could have been observed. As a time series is a stochastic process, each one could be entirely different. The realization Is the one that was actually observed. Sometimes a realization is the only realization possible. We can theorize what an infinite population of those realizations, but it is often the case that there is only one. An ensemble is the theoretical totality of all possible realizations. It is the ensemble of realizations. We can think of that as the population of realizations. When we are thinking of means and variances, we think of them in terms of the ensemble, even though we may only be able to look at a single realization. 1.2 Population Means and Variances It is important to note that when dealing with an ensemble of realizations, each time point is allowed to have its own mean and variance. 1.2.1 The Mean of \\(X_t\\) \\(\\mu_t\\) is the mean of all possible realizations of \\(X_t\\) for a fixed t. 1.2.2 Variance of \\(X_t\\) \\({\\sigma_t}^{2}\\) is the variance of all possible realizations of \\(X_t\\) for a fixed t 1.3 Estimating Mean and Variance for Multiple Realizations Let us say we have \\(n\\) realizations of \\(X_t\\) at time \\(t\\). The average can be then written as \\(\\hat{\\mu_t}=\\frac{1}{n}\\sum_{i=1}^{n}X_{t_n}\\) Variance is calculated the same way. We are making sample means and sample variances. If we assume the population is normally distributed, this allows us to take confidence intervals on the mean of our set of realizations. However, We have a problem: What to do when we only have one realization? 1.4 Expected Value In short, the expected value \\((E[X])\\) of a random variable (RV) denoted by \\(X\\) is the mean or more intuitively the long run average of the event that variable represents. 1.4.1 Discrete RVs A discrete RV is an RV in which \\(X\\) cannot take on any value, it has specific values it can exist at and that is it. The formula for EV of a discrete RV is as follows: \\[E\\left[X\\right] = \\sum X P\\left(X\\right) = \\mu\\] 1.4.2 Continuous RVs A continuous RV has instead of discrete values a probability distribution/density function, \\(f(x)\\). The formula for EV of a continuous RV is as follows: \\[E\\left[X\\right] = \\int_{a}^{b} xf\\left(x\\right)dx = \\mu\\] Note that is directly analagous to the discrete RV, and that a and b can span to \\(\\pm\\infty\\) Important Note \\[E\\left[X\\right] = \\mu\\] That is the expected value of a RV is equivalent to the population mean 1.4.3 Some Rules Let a and b be constants \\[E\\left[a\\right] = a\\] \\[E[aX] = aE[X] = a\\mu\\] \\[E[aX + b] = aE[X] + E[b] = aE[x] + b = a\\mu+b\\] Knowing these rules, lets try out a challenge: 1.4.4 A challenge in Expected Values Find \\(E[\\bar{x}]\\) where \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}{x_i}\\) E.G find the expected value of an average 1.4.5 A solution First, we can factor out the 1/n, so we have \\[\\frac{1}{n}E\\left[\\sum_{i=1}^{n}{x_i}\\right]\\] then we can say that the sum is just \\(x_1+x_2+x_3+ ... + x_n\\). This would lead us to the conclusion that \\[ E[\\bar{x}] =\\frac{1}{n}\\sum_{i=1}^{n}E\\left[x_i\\right]\\] If we let each \\(x_i\\) be a constant then we have: \\[ E[\\bar{x}] =\\frac{1}{n}\\sum_{i=1}^{n}x_i = \\bar{x}\\] This makes sense as \\(\\bar{x}\\) is the average value of x, and so the expected value of x for a very high n is the average value of x 1.5 Variance and Covariance 1.5.1 Variance The variance is the dispersion. Variance is defined as \\[\\mathrm{Var}\\left(X\\right) = E \\left[\\left(X-\\mu\\right)^2\\right] = \\int_{-\\infty}^{\\infty} \\left(x - \\mu\\right)^{2}f\\left(x\\right)dx = \\sigma^2\\] \\[\\widehat{\\mathrm{Var}\\left(X\\right) }= \\sum\\left(x_i - \\bar{x}\\right)^2\\] 1.5.2 Covariance Assume we have two RVs, \\(X\\) and \\(Y\\): \\[\\mathrm{Cov}(X,Y) = E\\left[\\left(X - \\mu_x\\right)\\left(Y - \\mu_y\\right)\\right]\\] \\[\\widehat{\\mathrm{Cov}\\left(X,Y\\right) }= \\sum\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)\\] The variance is, in essence, the sum of cross products. 1.5.3 Some generalizations on covariance: If y tends to increase with x, then \\(\\mathrm{Cov}(X,Y)&gt;0\\) If Y tends to decrease with X, then \\(\\mathrm{Cov}(X,Y)\\) is Less than 0 If it appears as a random cloud, then \\(\\mathrm{Cov}(X,Y)\\) is approximately 0 1.5.4 A challenge Let \\(X\\) be a RV. What is \\(\\mathrm{Cov}(X,X)\\)? 1.5.5 A solution \\[\\mathrm{Cov}(X,X) = E\\left[\\left(X - \\mu_x\\right)\\left(X - \\mu_X\\right)\\right]\\] We see that the arguments in the left and right parens are exactly the same. Thus, we can rewrite the equation as: \\[\\mathrm{Cov}(X,X) = E \\left[\\left(X - \\mu_x\\right)^2\\right] = \\mathrm{Var}(X) \\] 1.5.6 A note on covariance and correlation Correlation is covariance but standardized. \\[\\mathrm{Corr}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{SD}(X)\\mathrm{SD}(Y)}\\] \\[ =\\frac{E[X - \\mu_x)(Y-\\mu_y)]}{\\sigma_x \\sigma_y}\\] \\[ \\widehat{\\mathrm{Corr}(X,Y)} = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{ns_x s_y}\\] "],
["a-brief-discussion-of-stationarity.html", "Unit 2 A Brief Discussion of Stationarity 2.1 Condition One: Constant Mean 2.2 Condition Two: Constant Variance 2.3 Condition 3: Constant autocorrelation", " Unit 2 A Brief Discussion of Stationarity In order for a time series to be considered stationary, it must satisfy three conditions: Constant Mean with Time Constant Variance with Time Constant Autocorellation with Time 2.1 Condition One: Constant Mean Mean is constant with time. That is \\(E[X_t] = \\mu\\). Note the lack of a little tiny t in mu! It is indepoent of tine! That is stationarity condition number 1! Important result: If we assume constant mean, we can use all of the data to estimate the mean Here we can see after 5 realizations that the mean is clearly not constant with time library(tswge) a1 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(3, 0), vara = 3, plot = F) b1 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(3, 0), vara = 3, plot = F) c1 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(3, 0), vara = 3, plot = F) d1 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(3, 0), vara = 3, plot = F) e1 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(3, 0), vara = 3, plot = F) Now lets look at 5 realizations which come from the same realization, which might not have a constant mean: a2 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(runif(1, 0, 2 * pi), 0), vara = 3, plot = F) b2 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(runif(1, 0, 2 * pi), 0), vara = 3, plot = F) c2 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(runif(1, 0, 2 * pi), 0), vara = 3, plot = F) d2 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(runif(1, 0, 2 * pi), 0), vara = 3, plot = F) e2 = gen.sigplusnoise.wge(100, coef = c(5, 0), freq = c(0.1, 0), psi = c(runif(1, 0, 2 * pi), 0), vara = 3, plot = F) We cannot say for sure whether the mean is constant or not in this case! 2.2 Condition Two: Constant Variance Variance does not depend on time Variance is constant and finite* That is : \\[Var[X_t] = \\sigma^2 \\neq \\infty\\] If we can make this assumption, then we can use all of the data to make the variance. This is typically the hardest one to tell, and it will take some practice to see. 2.3 Condition 3: Constant autocorrelation Correlation of \\(X_{t_1} and X_{t_2}\\) only depends on \\(t_2 - t_1\\) That is, the correlation between points depends only on how far apart they are in time, not where they are in time. To describe this mathematically: Let \\[h = t_2 - t_1\\] then \\[\\mathrm{Cor}\\left(X_t,X_{t+h}\\right) = \\rho_h\\] \\(\\rho\\) represents the population correlation coefficient. "],
["autocorrelation.html", "Unit 3 Autocorrelation 3.1 Independence 3.2 Serial Dependence / Autocorrelation", " Unit 3 Autocorrelation library(tswge) library(ggplot2) acfdf &lt;- function(vec) { vacf &lt;- acf(vec, plot = F) with(vacf, data.frame(lag, acf)) } ggacf &lt;- function(vec) { ac &lt;- acfdf(vec) ggplot(data = ac, aes(x = lag, y = acf)) + geom_hline(aes(yintercept = 0)) + geom_segment(mapping = aes(xend = lag, yend = 0)) } tplot &lt;- function(vec) { df &lt;- data.frame(X = vec, t = seq_along(vec)) ggplot(data = df, aes(x = t, y = X)) + geom_point() + geom_line() } 3.1 Independence Two events are independent in a time series if the probability that an event at time t occurs in no way depends on the ocurrence of any event in the past or affects any event in the future. Mathematically, this is written as: \\[\\mathrm{Independence: }P \\left(x_{t+1}|X_t\\right) = P\\left(X_{t+1}\\right)\\] 3.2 Serial Dependence / Autocorrelation 3.2.1 A definition If two events are independent, their corellation is 0 That is if \\(X_t\\) and \\(X_{t+k}\\) are independent, \\(\\rho_{x_{t},x_{t+k}}=0\\) Corollary: If the correlation between two variables is not zero, then they are not independent In other words if \\(\\rho_{x_{t},x_{t+k}} \\neq 0\\), they are not independent. 3.2.2 Autocorrelation Plots 3.2.3 Dependent(ish) data In time series we look at the autocorrelation between \\(X_t\\) and \\(X_{t+1}\\) etc (with t and t+1 it is lag 1 autocorrelation) For example, visually, let us define a vector, \\(Y5\\) and take its autocorrelation: Y5 &lt;- c(5.1, 5.2, 5.5, 5.3, 5.1, 4.8, 4.5, 4.4, 4.6, 4.6, 4.8, 5.2, 5.4, 5.6, 5.4, 5.3, 5.1, 5.1, 4.8, 4.7, 4.5, 4.3, 4.6, 4.8, 4.9, 5.2, 5.4, 5.6, 5.5, 5.5) knitr::kable(acfdf(Y5)) lag acf 0 1.0000000 1 0.8268482 2 0.5166514 3 0.1125421 4 -0.2687774 5 -0.5538901 6 -0.6935454 7 -0.6979912 8 -0.5505557 9 -0.2752524 10 -0.0056082 11 0.2274804 12 0.3902111 13 0.4573468 14 0.4660549 tplot(Y5) + ggthemes::theme_few() ggacf(Y5) + ggthemes::theme_few() 3.2.4 Independent(ish) data! Now let us look at autocorrelation of independent data xs = gen.arma.wge(n = 250) Let’s check it out tplot(xs) + ggthemes::theme_few() ggacf(xs) + ggthemes::theme_few() We see that the autocorrelation is more or less zero "],
["autocorrelation-concepts-and-notation.html", "Unit 4 Autocorrelation Concepts and Notation 4.1 Theoretical concept of \\(\\rho\\) 4.2 autocovariance 4.3 autocorrelation 4.4 Stationary Covariance 4.5 Stationary Time Series: Putting it Together", " Unit 4 Autocorrelation Concepts and Notation 4.1 Theoretical concept of \\(\\rho\\) \\[\\rho = \\frac{E\\left[\\left(X-\\mu_X\\right)\\left(Y-\\mu_Y\\right)\\right]}{\\sigma_X \\sigma_Y} = \\frac{\\mathrm{covariance between X+y}}{\\sigma_X \\sigma_Y}\\] In a stationary time series, we have autocorrelation and autocovariance 4.2 autocovariance \\[\\gamma_k = E\\left[\\left(X_t - \\mu\\right)\\left(X_{t+k} - \\mu\\right)\\right]\\] 4.3 autocorrelation \\[\\rho_k =\\frac{E\\left[\\left(X_t - \\mu\\right)\\left(X_{t+k} - \\mu\\right)\\right]}{\\sigma_{X_t} \\sigma_{X_{t+k}}} = \\frac{E\\left[\\left(X_t - \\mu\\right)\\left(X_{t+k} - \\mu\\right)\\right]}{\\sigma^2_X} = \\frac{\\gamma_k}{\\sigma_X^2}\\] This works because of constant variance and constant mean! Note that in a stationary time series: \\[\\sigma_X^2 = 1E[(X_t - \\mu)^2] = E[(X_t -\\mu )(X_t - \\mu)] = \\gamma_0\\] Therefore: \\[\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\\] 4.4 Stationary Covariance Correlation is not affected by where, only by how far apart, that is: \\[ \\rho_h = \\mathrm{Cor} \\left(X_t, X_{t+h}\\right)\\] Let us try it out with this little snippet: ggsplitacf &lt;- function(vec) { h1 &lt;- vec[1:(length(vec)/2)] h2 &lt;- vec[(length(vec)/2):length(vec)] first &lt;- ggacf(vec) + ggthemes::theme_few() second &lt;- ggacf(h1) + ggthemes::theme_few() third &lt;- ggacf(h2) + ggthemes::theme_few() cowplot::plot_grid(first, second, third, labels = c(&quot;original&quot;, &quot;first half&quot;, &quot;second half&quot;), nrow = 2, align = &quot;v&quot;) } Realize = gen.arma.wge(500, 0.95, 0, plot = T, sn = 784) ggsplitacf(Realize) This looks pretty stationary to me! Let’s try with some sample data: data(&quot;noctula&quot;) tplot(noctula) + ggthemes::theme_few() ggsplitacf(noctula) So in this case our mean probably looks constant, our variance could be constant as well, however the ACFs are clearly different. Let’s look at a different dataset: data(lavon) tplot(lavon) + ggthemes::theme_few() ggsplitacf(lavon) In this case, our mean looks maybe to be constant around 495, our variance does not appear to be constant, and our ACF looks pretty good, however I would say overall this probably is not stationary, due to the variance and slightly off ACF. 4.5 Stationary Time Series: Putting it Together 4.5.1 Workflow: Step 1: Check condition 1 see if the assumption is more or less met Step 2: Check condition two same thing Step 3: Check for condition three If one of the things is iffy, and the other two arent, we are going to be just fine. "],
["practical-autocorrelation.html", "Unit 5 Practical Autocorrelation 5.1 Estimation!", " Unit 5 Practical Autocorrelation 5.1 Estimation! 5.1.1 \\(\\rho_k \\rightarrow 0\\) For a stationary time series, if the autocorrelation approaches zero, then : A Single realization lets us estimate mean, variance, and autovvariance! Remember that \\(\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\\), and we use n-k pairs to calculate it (the summation) 5.1.1.1 Mean Just calculate the mean normally for this case. 5.1.1.2 Variance \\[\\mathrm{Var}\\left(\\bar{X}\\right) = \\frac{\\sigma^2}{n} \\sum^{n-1}_{k = -(n-1)} \\left( 1 - \\frac{\\mid{k}\\mid}{n} \\right)\\rho_k\\] \\(\\sigma^2\\) is calculated as normal, we will see rhok next! remember ! Now it is time for some code! library(glue) ## ## Attaching package: &#39;glue&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse xbar &lt;- function(xs) { mean(xs) } ghat_zero &lt;- function(xs) { summand &lt;- (xs - xbar(xs))^2 mean(summand) } ghat_one &lt;- function(xs) { lhs &lt;- xs[1:(length(xs) - 1)] - xbar(xs) rhs &lt;- xs[2:length(xs)] - xbar(xs) summand &lt;- lhs * rhs summate &lt;- sum(summand) summate/length(xs) } rhohat_zero &lt;- 1 rhohat_one &lt;- function(xs) { ghat_one(xs)/ghat_zero(xs) } v &lt;- c(76, 70, 66, 60, 70, 72, 76, 80) xbar(v) ## [1] 71.25 ghat_zero(v) ## [1] 34.9375 ghat_one(v) ## [1] 14.74219 rhohat_one(v) ## [1] 0.4219589 "],
["the-frequency-domain.html", "Unit 6 The Frequency Domain 6.1 Review of sine and cosine 6.2 Frequency and Period 6.3 psueudo-periodicity 6.4 Aperiodicity 6.5 Frequency 6.6 Frequency Terminology 6.7 Composite sine function", " Unit 6 The Frequency Domain 6.1 Review of sine and cosine 6.1.1 Phase shifts This is what a sine wave looks like \\[\\delta = 0; y = \\mathrm(sin)(t)\\] We can phase shift the sin wave by adding RED: \\[\\delta = \\pi; y = \\mathrm{sin}(t+\\pi)\\] BLUE: \\[\\delta = \\frac{\\pi}{2}; y = \\mathrm{sin}(t+\\frac{\\pi}{2})\\] Now cosine 6.2 Frequency and Period Let us say \\(f(x)\\) is a periodic function. If \\(f(x)\\) has period \\(p\\), then \\(p\\) is the smallest value S.T. \\[f(x) = f(x + kp) \\] Cool 6.3 psueudo-periodicity Data are psuedo-periodic with period \\(p\\) if \\(p\\) is the smallest value such that a cycle appears to repeat itself. 6.4 Aperiodicity Aperiodic data is data in which no \\(p\\) exists. 6.5 Frequency Frequency is calculated as : \\(\\frac{1}{p}\\) It symbolizes the number of cycles per unit(number of periods per unit) 6.6 Frequency Terminology Frequency of a trigonometruc function: Let \\(f(x)=\\sin\\left( 2\\pi f t \\right)\\) Then we have that : \\[p = 1/f\\] \\[\\nu = f\\] Consider: \\[\\sin(Bt + C)\\] Then \\[p = \\frac{2 \\pi}{B}\\] \\[\\nu = \\frac{B} {2 \\pi}\\] 6.7 Composite sine function t &lt;- seq(1, 100, length = 100) y1 &lt;- sin(2 * pi * 0.025 * t) y2 &lt;- sin(2 * pi * 0.1 * t + 1) y3 &lt;- sin(2 * pi * 0.15 * t + 2.5) lineplotter &lt;- function(x, y) { plot(x, y, type = &quot;l&quot;) } lineplotter(t, y1) lineplotter(t, y2) lineplotter(t, y3) lineplotter(t, y1 + y2 + y3) this is a signal with three frequencies! "]
]
